---
layout: post
title: "From Missteps to Mastery: Building Hashrate Distribution Logic in DMND"
date: 2025-06-18
author: Ayush Bansal
categories: [Demand, Stories]
---

> â€œThe best way to learn how to build the right thingâ€¦ is by building the
> wrong thing first.â€

Over the last few weeks, Iâ€™ve been contributing to Demand CLI (DMND), an open-
source Stratum V2 proxy built in Rust for Bitcoin miners. What seemed like a
simple feature request â€” distributing mining workload across multiple upstream
pools â€” turned into one of the most educational and humbling experiences Iâ€™ve
had as an open-source contributor.

In this post, Iâ€™ll walk you through:

  * What I initially built (and why it was wrong),
  * The discussion with my mentor that changed everything,
  * How I fixed it with a clean architecture and dynamic assignment logic,
  * And the lessons Iâ€™m carrying forward from this experience.

# ğŸŒ What is Demand CLI?

Demand CLI is a lightweight Rust-based proxy that allows Bitcoin miners to
connect to the **Demand Pool** , even if their hardware still speaks the older
Stratum V1 protocol. It handles:

  * ğŸ§  Protocol translation (Stratum V1 âŸ¶ Stratum V2)
  * ğŸ› ï¸ Job Declaration (letting miners propose block templates)
  * âš¡ Performance improvements with modern async networking

In short, it makes it easier for legacy miners to participate in modern,
decentralized mining with full-featured coordination.

# âŒ PR #71: When You Build the Wrong Thing (Even If It Works)

The original feature request was to allow **multi-upstream pool support** â€”
where miners can split their effort across different mining pool endpoints.

So I did what I thought was logical:

> _â€œLetâ€™s split the hashrate. If a miner has 100 TH/s and the user configures
> a 70:30 split between Pool A and Pool B, Iâ€™ll send 70 TH/s to A and 30 TH/s
> to B.â€_

I implemented this logic in PR #71. It compiled. It ran. I even wrote solid
routing logic and connection handling.

But it was wrong.

# ğŸ’£ The Problem

A miner can only open **one connection at a time**. Thereâ€™s no way to â€œsplitâ€
a minerâ€™s hashrate across multiple pools â€” at least not without firmware-level
changes that werenâ€™t feasible.

Instead, the real requirement was something completely different.

# ğŸ’¡ Mentor to the Rescue: â€œYouâ€™re Splitting Hashrate â€” But You Should Be
Splitting Minersâ€

My mentor helped me realize the core misunderstanding during a quick Gmeet
session:

> _â— We donâ€™t want to split the_ hashrate _of individual miners.  
>  âœ… We want to distribute the _miners themselves _across multiple upstream
> pools based on configured percentages._

For example:

> _If you have 10 miners and configure a 70:30 split â†’  
>  â†’ 7 miners should connect to Pool A  
>  â†’ 3 miners should connect to Pool B_

Each miner should connect entirely to **one** pool â€” and we aim to maintain
the configured distribution across the whole fleet.

This clarity changed everything.

# ğŸ§± The Rebuild: PR #100 and a Fresh Architecture

I scrapped the earlier approach and started from scratch in PR #100. This new
version correctly implemented **miner-based distribution** , using a cleaner,
smarter architecture.

Hereâ€™s the configuration format:

    
    
    pool_addresses = [  
      "stratum+tcp://pool1.example.com:3333",  
      "stratum+tcp://pool2.example.com:4444"  
    ]
    
    
    hashrate_distribution = [70.0, 30.0] # 70% to Pool1, 30% to Pool2

Now, when miners start up, they are **assigned to one of the pools based on
this distribution** , and the system keeps track of how many miners are
connected to each one.

# âš™ï¸ Smart Dynamic Assignment in Action

Instead of statically assigning the first `n` miners to Pool1 and the rest to
Pool2, I wrote a **dynamic routing algorithm** that adjusts in real time.

Hereâ€™s a sample output:

    
    
    // Miner 1: Pool1 needs 70%, â†’ Pool1 selected  
    // Miner 2: Pool1 has 100%; Pool2 needs 30% â†’ Pool2 selected  
    // Miner 3: Pool1 has 50%; Pool2 has 50% â†’ Pool1 selected  
    // Miner 4: Pool1 has 66.66%; Pool2 has 33.33% â†’ Pool1 selected  
    // Miner 5: Pool1 has 75%; Pool2 has 25% â†’ Pool2 selected

âœ¨ This ensures:

  * Pool shares are respected **even as miners come online out-of-order**.
  * No single pool gets overloaded.
  * Distribution is smooth and proportional.

The routing logic checks current miner counts for each pool, compares with the
target percentages, and routes each new miner to the pool thatâ€™s furthest
behind its goal.

# ğŸ§  Key Takeaways

# 1\. Understand Before You Implement

I jumped into code with a vague idea of what â€œmulti-pool supportâ€ meant. That
cost time and effort. Now, I always **discuss the architecture with my
mentor** before writing code.

# 2\. Dynamic Is Better Than Static

By implementing a real-time assignment system, we gained flexibility,
resilience, and smarter resource utilization â€” which is essential for real-
world mining farms.

# 3\. Mistakes Make You Better

My first PR was wrong. It was closed. But that process taught me more than if
I had shipped the right thing on the first try. Thatâ€™s open source â€” learning
in public, and getting better with each iteration.

# ğŸ§­ Final Thoughts

The PR is still a **work in progress** , but the direction is now correct â€”
and Iâ€™m confident itâ€™ll soon be merged.

If youâ€™re new to open source or systems programming, let me leave you with
this:

  * Ask early.
  * Validate your understanding.
  * Donâ€™t be afraid to scrap and rebuild.
  * Communicate often with your mentor or team.

And remember: **getting it wrong at first is often the fastest way to get it
right in the end.**

ğŸ™Œ Thanks to my mentor for helping me grow through this process. If youâ€™re
working on mining protocols, Rust projects, or just love contributing to infra
â€” letâ€™s connect!